---
title: Upgrade from agents to workers
description: Learn how to upgrade from agents to workers to significantly enhance the experience of deploying flows.
---

Upgrading from agents to workers significantly enhances the experience of deploying flows 
by simplifying the specification of each flow's infrastructure and runtime environment.

<Note>
This guide is for users who are on `prefect<3.0` who are upgrading from agents to workers.
If you are new to Prefect, we recommend starting with the 
[Prefect Quickstart](/3.0/get-started/quickstart/).
</Note>

## About workers and agents
A [worker](/3.0/deploy/infrastructure-concepts/workers/) is the fusion of an 
agent with an infrastructure block. 
Like agents, workers poll a work pool for flow runs that are scheduled to start. 
Like infrastructure blocks, workers are typed. They work with only one kind of infrastructure, 
and they specify the default configuration for jobs submitted to that infrastructure.

Accordingly, workers are not a drop-in replacement for agents. **Using workers requires 
deploying flows differently.** In particular, deploying a flow with a worker does not involve 
specifying an infrastructure block. Instead, infrastructure configuration is specified on the 
[work pool](/3.0/deploy/infrastructure-concepts/work-pools/) and passed to each worker that polls work 
from that pool.

---
title: What's new in Prefect 3.0
sidebarTitle: What's new
---

Prefect 3.0 introduces a number of enhancements to the OSS product: a new events & automations backend for event-driven workflows and observability, improved runtime performance, autonomous task execution and a streamlined caching layer based on transactional semantics.

Most Prefect 2.0 users can upgrade without changes to their existing workflows. Please review the [upgrade guide](/3.0/resources/upgrade-to-prefect-3) for more information.

<Info>
**Prefect 2.0** refers to the 2.x lineage of the open source prefect package, and **Prefect 3.0** refers exclusively to the 3.x lineage of the prefect package. Neither version is strictly tied to any aspect of Prefect's commercial product, [Prefect Cloud](/3.0/manage/cloud). 
</Info>

## Open source events and automation system

One of the largest features in Prefect 3.0 is the introduction of the events and automation system to the open source package. Previously exclusive to Prefect Cloud, this system now allows all users to create event-driven workflows and automate their system based on the presence or absence of observable events. 

With this new capability, you can trigger actions based on specific event payloads, cancel runs if certain conditions aren't met, or automate workflow runs based on external events. For instance, you could initiate a data processing pipeline automatically when a new file lands in an S3 bucket. The system also enables you to receive notifications for various system health events, giving you greater visibility and control over your workflows.

## New transactional interface

Another major addition in Prefect 3.0 is the new transactional interface. This powerful feature makes it easier than ever to build resilient and idempotent pipelines. With the transactional interface, you can group tasks into transactions, automatically roll back side effects on failure, and significantly improve your pipeline's idempotency and resilience.

For example, you can define rollback behaviors for your tasks, ensuring that any side effects are cleanly reversed if a transaction fails. This is particularly useful for maintaining data consistency in complex workflows involving multiple steps or external systems.

## Flexible task execution

Prefect 3.0 has no restrictions on where tasks can run. Tasks can be nested within other tasks, allowing for more flexible and modular workflows; they can also be called outside of flows, essentially enabling Prefect to function as a background task service. You can now run tasks autonomously, apply them asynchronously, or delay their execution as needed. This flexibility opens up new possibilities for task management and execution strategies in your data pipelines.

## Enhanced client-side engine

Prefect 3.0 comes with a thoroughly reworked client-side engine that brings several improvements to the table. You can now nest tasks within other tasks, adding a new level of modularity to your workflows. The engine also supports generator tasks, allowing for more flexible and efficient handling of iterative processes.

One of the most significant changes is that all code now runs on the main thread by default. This change improves performance and leads to more intuitive behavior, especially when dealing with shared resources or non-thread-safe operations.

## Improved artifacts and variables

Prefect 3.0 enhances the artifacts system with new types, including progress bars and image artifacts. These additions allow for richer, more informative task outputs, improving the observability of your workflows.

The variables system has also been upgraded to support arbitrary JSON, not just strings. This expansion allows for more complex and structured data to be stored and retrieved as variables, increasing the flexibility of your workflow configurations.

## Workers

Workers were first introduced in Prefect 2.0 as next-generation agents, and are now standard in Prefect 3.0. Workers offer a stronger governance model for infrastructure, improved monitoring of jobs and work pool/queue health, and more flexibility in choosing compute layers, resulting in a more robust and scalable solution for managing the execution of your workflows across various environments.

Worker logs are now sent to the API only if a worker ID is present, indicating a connection to Prefect Cloud. This change simplifies the logging configuration and ensures logs are only sent when supported by the backend. The `PREFECT_EXPERIMENTS_WORKER_LOGGING_TO_API_ENABLED` environment variable has been removed, and users should update their configurations to align with the new logging behavior.

## Performance enhancements

Prefect 3.0 doesn't just bring new features; it also delivers significant performance improvements. Users running massively parallel workflows on distributed systems such as Dask and Ray will notice substantial speedups. In some benchmark cases, we've observed up to a 98% reduction in runtime overhead. These performance gains translate directly into faster execution times and more efficient resource utilization for your data pipelines.

## Release notes

See release notes for each released version in the [Gitub repository](https://github.com/PrefectHQ/prefect/releases).

---
title: What's new in Prefect 3.0
sidebarTitle: What's new
---

Prefect 3.0 introduces a number of enhancements to the OSS product: a new events & automations backend for event-driven workflows and observability, improved runtime performance, autonomous task execution and a streamlined caching layer based on transactional semantics.

Most Prefect 2.0 users can upgrade without changes to their existing workflows. Please review the [upgrade guide](/3.0/resources/upgrade-to-prefect-3) for more information.

<Info>
**Prefect 2.0** refers to the 2.x lineage of the open source prefect package, and **Prefect 3.0** refers exclusively to the 3.x lineage of the prefect package. Neither version is strictly tied to any aspect of Prefect's commercial product, [Prefect Cloud](/3.0/manage/cloud). 
</Info>

## Open source events and automation system

One of the largest features in Prefect 3.0 is the introduction of the events and automation system to the open source package. Previously exclusive to Prefect Cloud, this system now allows all users to create event-driven workflows and automate their system based on the presence or absence of observable events. 

With this new capability, you can trigger actions based on specific event payloads, cancel runs if certain conditions aren't met, or automate workflow runs based on external events. For instance, you could initiate a data processing pipeline automatically when a new file lands in an S3 bucket. The system also enables you to receive notifications for various system health events, giving you greater visibility and control over your workflows.

## New transactional interface

Another major addition in Prefect 3.0 is the new transactional interface. This powerful feature makes it easier than ever to build resilient and idempotent pipelines. With the transactional interface, you can group tasks into transactions, automatically roll back side effects on failure, and significantly improve your pipeline's idempotency and resilience.

For example, you can define rollback behaviors for your tasks, ensuring that any side effects are cleanly reversed if a transaction fails. This is particularly useful for maintaining data consistency in complex workflows involving multiple steps or external systems.

## Flexible task execution

Prefect 3.0 has no restrictions on where tasks can run. Tasks can be nested within other tasks, allowing for more flexible and modular workflows; they can also be called outside of flows, essentially enabling Prefect to function as a background task service. You can now run tasks autonomously, apply them asynchronously, or delay their execution as needed. This flexibility opens up new possibilities for task management and execution strategies in your data pipelines.

## Enhanced client-side engine

Prefect 3.0 comes with a thoroughly reworked client-side engine that brings several improvements to the table. You can now nest tasks within other tasks, adding a new level of modularity to your workflows. The engine also supports generator tasks, allowing for more flexible and efficient handling of iterative processes.

One of the most significant changes is that all code now runs on the main thread by default. This change improves performance and leads to more intuitive behavior, especially when dealing with shared resources or non-thread-safe operations.

## Improved artifacts and variables

Prefect 3.0 enhances the artifacts system with new types, including progress bars and image artifacts. These additions allow for richer, more informative task outputs, improving the observability of your workflows.

The variables system has also been upgraded to support arbitrary JSON, not just strings. This expansion allows for more complex and structured data to be stored and retrieved as variables, increasing the flexibility of your workflow configurations.

## Workers

Workers were first introduced in Prefect 2.0 as next-generation agents, and are now standard in Prefect 3.0. Workers offer a stronger governance model for infrastructure, improved monitoring of jobs and work pool/queue health, and more flexibility in choosing compute layers, resulting in a more robust and scalable solution for managing the execution of your workflows across various environments.

Worker logs are now sent to the API only if a worker ID is present, indicating a connection to Prefect Cloud. This change simplifies the logging configuration and ensures logs are only sent when supported by the backend. The `PREFECT_EXPERIMENTS_WORKER_LOGGING_TO_API_ENABLED` environment variable has been removed, and users should update their configurations to align with the new logging behavior.

## Performance enhancements

Prefect 3.0 doesn't just bring new features; it also delivers significant performance improvements. Users running massively parallel workflows on distributed systems such as Dask and Ray will notice substantial speedups. In some benchmark cases, we've observed up to a 98% reduction in runtime overhead. These performance gains translate directly into faster execution times and more efficient resource utilization for your data pipelines.

## Release notes

See release notes for each released version in the [Gitub repository](https://github.com/PrefectHQ/prefect/releases).

## What's similar

- You can set storage blocks as the pull action in a `prefect.yaml` file.
- Infrastructure blocks have configuration fields similar to typed work pools.
- Deployment-level infrastructure overrides operate in much the same way.

    `infra_override` -> [`job_variable`](/3.0/deploy/infrastructure-concepts/prefect-yaml/#work-pool-fields)

- The process for starting an agent and [starting a worker](/3.0/deploy/infrastructure-concepts/workers/#start-a-worker) 
in your environment are virtually identical.

    `prefect agent start --pool <work pool name>` --> `prefect worker start --pool <work pool name>`
<Tip>
**Worker Helm chart**

If you host your agents in a Kubernetes cluster, you can use the [Prefect worker Helm chart](https://github.com/PrefectHQ/prefect-helm/tree/main/charts/prefect-worker) 
to host workers in your cluster.
</Tip>

## Upgrade steps

If you have existing deployments that use infrastructure blocks, you can quickly upgrade them to 
be compatible with workers by following these steps:

1. **[Create a work pool](/3.0/deploy/infrastructure-concepts/work-pools/#work-pool-configuration)**

This new work pool replaces your infrastructure block.

You can use the [`.publish_as_work_pool`](https://docs.prefect.io/2.19.2/api-ref/prefect/infrastructure/#prefect.infrastructure.Infrastructure.publish_as_work_pool) 
method on any infrastructure block to create a work pool with the same configuration.

{/*
<!-- vale off -->
*/}

For example, if you have a `KubernetesJob` infrastructure block named 'my-k8s-job', you can 
create a work pool with the same configuration with this script:

```python
from prefect.infrastructure import KubernetesJob


KubernetesJob.load("my-k8s-job").publish_as_work_pool()
```

Running this script creates a work pool named 'my-k8s-job' with the same configuration as your infrastructure block.

{/*
<!-- vale on -->
*/}


<Tip>
**Serving flows**
If you are using a `Process` infrastructure block and a `LocalFilesystem` storage block 
(or aren't using an infrastructure and storage block at all), you can use [`flow.serve`](/3.0/deploy/index) 
to create a deployment without specifying a work pool name or start a worker.

This is a quick way to create a deployment for a flow and manage your 
deployments if you don't need the dynamic infrastructure creation or configuration offered 
by workers.
</Tip>

2. **[Start a worker](/3.0/deploy/infrastructure-concepts/workers/#start-a-worker)**

This worker replaces your agent and polls your new work pool for flow runs to execute.

```bash
prefect worker start -p <work pool name>
```

3. **Deploy your flows to the new work pool**

To deploy your flows to the new work pool, use `flow.deploy` for a Pythonic deployment 
experience or `prefect deploy` for a YAML-based deployment experience.

If you currently use `Deployment.build_from_flow`, we recommend using `flow.deploy`.

If you currently use `prefect deployment build` and `prefect deployment apply`, we recommend 
using `prefect deploy`.

### Use `flow.deploy`

If you have a Python script that uses `Deployment.build_from_flow` to create a deployment, you
can replace it with `flow.deploy`.

You can translate most arguments to `Deployment.build_from_flow` directly to `flow.deploy`, 
but here are some possible changes you may need:

- Replace `infrastructure` with `work_pool_name`.
  - If you've used the `.publish_as_work_pool` method on your infrastructure block, use the 
  name of the created work pool.
- Replace `infra_overrides` with `job_variables`.
- Replace `storage` with a call to [`flow.from_source`](/3.0/deploy/index).
  - `flow.from_source` loads your flow from a remote storage location and makes it deployable. 
  You can pass your existing storage block to the `source` argument of `flow.from_source`.

Below are some examples of how to translate `Deployment.build_from_flow` into `flow.deploy`.

#### Deploying from a local file

Using agents and `Deployment.build_from_flow` to deploy a flow from a local file looked like:

```python
from prefect import flow


@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a Python script!")


if __name__ == "__main__":
    Deployment.build_from_flow(
        my_flow,
        name="my-deployment",
        parameters=dict(name="Marvin"),
    )
```

When using workers, you can accomplish the same local-storage deployment with `flow.deploy`:

```python example.py
from pathlib import Path
from prefect import flow


@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a Python script!")


if __name__ == "__main__":
    my_flow.from_source(
        source=str(Path(__file__).parent),
        entrypoint="example.py:my_flow",
    ).deploy(
        name="my-deployment",
        parameters=dict(name="Marvin"),
        work_pool_name="local",
    )
```

You can then start a worker to execute scheduled runs, pulling the flow code from `example.py`:

```bash
# starts a worker and creates `local` Process work pool if it doesn't exist
prefect worker start --pool local
```

<Note>
If you'd like to immediately serve this flow as a deployment without running a worker or using work pools, you can [use `flow.serve`](/3.0/deploy/run-flows-in-local-processes/).
</Note>

#### Deploying using a storage block

If you currently use a storage block to load your flow code but no infrastructure block:

```python
from prefect import flow
from prefect.storage import GitHub

  
@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a GitHub repo!")


if __name__ == "__main__":
    Deployment.build_from_flow(
        my_flow,
        name="my-deployment",
        storage=GitHub.load("demo-repo"),
        parameters=dict(name="Marvin"),
    )
```

You can use `flow.from_source` to load your flow from the same location and `flow.deploy` to 
create a deployment:

```python example.py
from prefect import flow
from prefect.runner.storage import GitRepository

@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a GitHub repo!")

if __name__ == "__main__":
    flow.from_source(
        source=GitRepository(
            url="https://github.com/me/myrepo.git",
            credentials={"username": "oauth2", "access_token": "my-access-token"},
        ),
        entrypoint="example.py:my_flow"
    ).deploy(
        name="my-deployment",
        parameters=dict(name="Marvin"),
        work_pool_name="local", # or the name of your work pool
    )
```

#### Deploy using an infrastructure block and a storage block

For the code below, you need to create a work pool from your infrastructure block and pass it to 
`flow.deploy` as the `work_pool_name` argument. You also need to pass your storage block to 
`flow.from_source` as the `source` argument.

```python example.py
from prefect import flow
from prefect.deployments import Deployment
from prefect.filesystems import GitHub
from prefect.infrastructure.kubernetes import KubernetesJob


@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a GitHub repo!")


if __name__ == "__main__":
    Deployment.build_from_flow(
        my_flow,
        name="my-deployment",
        storage=GitHub.load("demo-repo"),
        entrypoint="example.py:my_flow",
        infrastructure=KubernetesJob.load("my-k8s-job"),
        infra_overrides=dict(pull_policy="Never"),
        parameters=dict(name="Marvin"),
    )
```

The equivalent deployment code using `flow.deploy` should look like this:

```python example.py
from prefect import flow
from prefect.storage import GitHub

if __name__ == "__main__":
    flow.from_source(
        source=GitHub.load("demo-repo"),
        entrypoint="example.py:my_flow"
    ).deploy(
        name="my-deployment",
        work_pool_name="my-k8s-job",
        job_variables=dict(pull_policy="Never"),
        parameters=dict(name="Marvin"),
    )
```
{/*
<!-- vale off -->
*/}

<Note>
When using `flow.from_source().deploy()` with a remote `source`such as a `GitHub` block or `str` URL like https://github.com/me/myrepo.git), the flow you're deploying doesn't need to be available locally before running your script.
See the [SDK reference](https://prefect-python-sdk-docs.netlify.app/prefect/#prefect.Flow.from_source) for more info on `from_source`.
</Note>

{/*
<!-- vale on -->
*/}

#### Deploy via a Docker image

If you currently bake your flow code into a Docker image before deploying, you can use the 
`image` argument of `flow.deploy` to build a Docker image as part of your deployment process:

```python
from prefect import flow

@flow(log_prints=True)
def my_flow(name: str = "world"):
    print(f"Hello {name}! I'm a flow from a Docker image!")


if __name__ == "__main__":
    my_flow.deploy(
        name="my-deployment",
        image="my-repo/my-image:latest",
        work_pool_name="my-k8s-job",
        job_variables=dict(pull_policy="Never"),
        parameters=dict(name="Marvin"),
    )
```

You can skip a `flow.from_source` call when building an image with `flow.deploy`. Prefect 
keeps track of the flow's source code location in the image and loads it from that location when the 
flow is executed.

### Use `prefect deploy`

<Warning>
**Always run `prefect deploy` commands from the `root` level of your repo!**

With agents, you may have multiple `deployment.yaml` files. But under worker deployment 
patterns, each repo has a single `prefect.yaml` file located at the **root** of the repo 
that contains [deployment configuration](/3.0/deploy/infrastructure-concepts/prefect-yaml/#work-with-multiple-deployments-with-prefect-yaml) 
for all flows in that repo.
</Warning>

To set up a new `prefect.yaml` file for your deployments, run the following command from the root 
level of your repo:

```bash
prefect deploy
```

This starts a wizard that guides you through setting up your deployment.

<Note>
**For step 4, select `y` on the last prompt to save the configuration for the deployment.**

Saving the configuration for your deployment results in a `prefect.yaml` file populated 
with your first deployment. You can use this YAML file to edit and [define multiple deployments](/3.0/deploy/infrastructure-concepts/prefect-yaml/#work-with-multiple-deployments-with-prefect-yaml) 
for this repo.
</Note>

You can add more [deployments](/3.0/deploy/infrastructure-concepts/prefect-yaml/#deployment-declaration-reference) 
to the `deployments` list in your `prefect.yaml` file and/or by continuing to use the deployment 
creation wizard.

For more information on deployments, check out our [in-depth guide for deploying flows to work pools](/3.0/deploy/infrastructure-examples/docker/).